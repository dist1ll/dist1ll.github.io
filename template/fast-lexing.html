<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Beating the fastest lexer generator in Rust</title>
		<link rel="canonical" href="https://alic.dev/blog/fast-lexing"/>
		<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
		<meta charset="UTF-8">
		<meta name="description" content="This blog post takes a look at a popular Rust crate for generating lexers, and my attempt to outperform it.">
		<meta name="theme-color" content="#000000">
		<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
		<link rel="stylesheet" href="../style.css">
		<link rel="stylesheet" href="../blogstyle.css">

		<link as="font" href="../static/fonts/cmu1.woff2" rel="preload" type="font/woff2">
		<link as="font" href="../static/fonts/cmu2.woff2" rel="preload" type="font/woff2">
		<link as="font" href="../static/fonts/cmu3.woff2" rel="preload" type="font/woff2">

		<style>
		@font-face {
			font-family: 'Computer Modern'; src: url('../static/fonts/cmu1.woff2') format('woff2');
			font-weight: 400; font-style: normal; font-display: swap;
		}
		@font-face {
			font-family: 'Computer Modern'; src: url('../static/fonts/cmu2.woff2') format('woff2');
			font-weight: 400; font-style: italic; font-display: swap;
		}
		@font-face {
			font-family: 'Computer Modern'; src: url('../static/fonts/cmu3.woff2') format('woff2');
			font-weight: 700; font-style: normal; font-display: swap;
		}
		</style>
	</head>
	<body>
		<div id="app">
			<div id="content">
				<div id="header">
						<a class="header-sel" href="https://alic.dev" style="margin-left: 5px;">Blog</a>
						<a class="header-def" href="https://alic.dev/repos.html">Repos</a>
						<a class="header-def" href="https://github.com/dist1ll">Github</a>
						<a class="header-def" href="https://www.linkedin.com/in/dist1ll/">Linkedin</a>
				</div>
				<div id="blog-title">Beating the Fastest Lexer Generator in Rust</div>
				<div class="blog-date">Jun 2023 - RSS<a href="../feed.xml"><img class="blog-rss"></a></div>
				
				<div id="blog-content">

					I was recently made aware of a crate for writing efficient lexers in Rust 
					called 
					<a href="https://crates.io/crates/logos"><span class="ilc">logos</span></a>. 
					According to its documentation, the project has two main goals.

						<ul>
							<li>To make it easy to create a lexer, so you can focus on more complex problems.</li>
							<li>To make the generated lexer faster than anything you'd write by hand.</li>
						</ul>

					<p>
					
					I had my doubts about the second part. I was aware of the efficiency of 
					state machine driven lexers, but most generators have one problem: they 
					can't be arbitrarily generic <b>and</b> consistently optimal at the same time. 
					There will always beÂ <i>some</i> assumptions about your data that are either 
					impossible to express, or outside the scope of the generator's optimizations. 
					Either way, I was curious to find out how my hand-rolled implementation 
					would fare.
					</p>

					<div class="blog-subtitle">State Machine Approach</div>

					Before we begin, I should mention what makes logos so fast in the first 
					place. Many lexer generators like logos compile a token specification 
					down to a jump table driven state machine. Every token variant is 
					represented as a state, each of which defines transitions for a feasible 
					subset of all possible scanned characters (usually ASCII or latin).  

					<figure>
						<div class="blog-img blog-imgwide">
							<img alt="Visualization of finite state machine" src="../static/fast-lexing/statemachine.webp" style="max-height: 800px">
						</div>					
					</figure>

					<p>
					Each invocation of <span class="ilc">next_token()</span> begins at the 
					start state, continually performing state transitions by looking up the 
					next character, until we hit a terminal state. The terminal state 
					contains the information about the token variant. If the number of 
					language keywords is sufficiently small, we could even build a trie 
					for matching against a static set of keywords. 
					</p>

					<p>
					The implementation of the state lookup is input-dependent. For ASCII 
					input, we can let each state define a full jump table for every possible 
					character, and use the raw bytes as lookup indices into the state 
					transition matrix. To handle full UTF-8 input, we might dispatch to a 
					more costly, branchy function with a straightforward switch statement. 
					</p>

					<p>
					For more resources, this <a href="http://nothings.org/computer/lexing.html">
					article by Sean Barret</a> gives a good overview of implementing a 
					table-driven lexer in C, and ways to improve performance via e.g. 
					equivalence classes. I also recommend a blog post on
					<a href="https://maciej.codes/2020-04-19-stacking-luts-in-logos.html">
					stacking lookup tables</a> by Maciej Hirsz, the author of logos.
					</p>


					<div class="blog-subtitle">Starting Point</div>
					<p>
					Since I'm working on a compiler in my free time, I already have a basic 
					lexer available. Lexing usually takes up the least amount of time in 
					modern compilers, so I prioritized simplicity over performance in my 
					implementation. One exception to this is keyword matching, for which 
					I used a perfect hash function. 
					</p>

					<span class="sidenote">
					<b>Note</b>: Compiler bottlenecks are pretty varied. For monomorphizing 
					compilers that emit native binaries, the bottleneck is more often 
					in the optimizer and codegen stage. 
					I've uploaded some 
					<a href="https://gist.github.com/dist1ll/8ceaa14211883044e375a26fb88efac9">Rust compiler timings</a> 
					for the <span class="ilc">regex</span> crate 
					As expected, the frontend is pretty lightweight, and significant time 
					is spent on the optimization passes, as well as generating the LLVM IR 
					and native code. But that is <a href="https://web.archive.org/web/20131127135422/https://www.drdobbs.com/cpp/increasing-compiler-speed-by-over-75/240158941">
					not always the case</a>!
					</span>

					<p>
					Still, it would be interesting to see by how much logos outperforms my 
					naive implementation. The target file was 100KB of pseudocode 
					(around ~8k LoC) and I ran the test on an Apple M1 system:
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      312,837 ns/iter (+/- 4,952)
logos:     174,476 ns/iter (+/- 3,062)
```
</div>
					</div>
					<p>
					Credit where credit is due, that is a pretty big performance difference! 
					Logos chews through the file in 174us - almost twice as fast as our 
					naive implementation. However, things are different on x86_64: 
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      201,148 ns/iter (+/- 27,477)
logos:     221,110 ns/iter (+/- 36,000)
```
</div>
					</div>

					<p>
					My first guess is that this is caused by differences in the speculative 
					execution architecture. If you compare <span class="ilc">perf</span> 
					reports of both lexers, you can see drastic differences in IPC and 
					branch counts. 

					<span class="sidenote">
					<b>Note</b>: This discrepancy between control-flow vs. data-dependency 
					was discussed in a recent HN post on 
					<a href="https://news.ycombinator.com/item?id=35737862">
					branchless binary search</a>. One thing I learned about speculative 
					execution is that <a href="https://arxiv.org/abs/1509.05053">it can 
					act as a more aggressive prefetcher</a> - which becomes important when 
					you're dealing with working set sizes beyond L1 or L2. Branchless code 
					often trades control-flow for data dependencies, but most CPUs will not 
					speculate on data for security reasons.
					</span>

					Other than that, the main contributor to our implementation's performance 
					is the perfect hash function, which we'll discuss next.
					</p>

					<div class="blog-subtitle">Keyword Matching with Perfect Hash Function</div>
					<p>
					Let's take a look at the list of keywords we are currently matching against:
					</p>

					<div class="blog-cg">
<div class="blog-code">
```rust
pub enum Keyword { 
    Class, For, Fn, Let, Match,
    Struct, Ln, While,
}
```
</div>
					</div>

					<p>
					One of the reasons our naive implementation can already beat logos is 
					that I can exploit the fact that all keywords are less than 8 bytes 
					long - so each keyword fits into a single 64-bit register. That means 
					that keyword comparisons can be done with a single <span class="ilc">cmp</span>
					instruction. 
					<span class="sidenote">
					<b>Note</b>: No need for transmutes here. Comparing appropriately-sized 
					byte arrays are a zero-cost abstraction when compiled with 
					<span class="ilc">--release</span>. For example, comparing two 
					<span class="ilc">[u8; 8]</span> on x86_64 yields 
					<span class="ilc">cmp rdi, rsi; sete al</span>.
					</span>
					</p>

					<p>
					To avoid comparing against all keywords, it's pretty common to statically 
					generate a <a href="https://en.wikipedia.org/wiki/Perfect_hash_function">
					perfect hash function</a>. You can do this with 
					<a href="https://www.gnu.org/software/gperf/">GNU's gperf</a> or just 
					experiment with it on your own. Ideally, such a hash function has a 
					super shallow data dependency tree and makes use of simple 
					instructions (*). 
					<span class="sidenote">
					(*): Wojciech Mula wrote <a href="http://0x80.pl/notesen/2023-04-30-lookup-in-strings.html">
					an article</a> about an efficient perfect hash implementation based 
					on bit extract instructions (e.g. <span class="ilc">pext</span> on x86, 
					but similar ones exist on other architectures).
					</span>
					Examples of parsers that make use of this include 
					<a href="https://news.ycombinator.com/item?id=18881403">Google's V8</a> 
					and <a href="https://www.postgresql.org/message-id/flat/E1ghOVt-0007os-2V%40gemulon.postgresql.org">PostgreSQL</a>. 
					However, being faster only on x86 is not enough - we also need to beat 
					<span class="ilc">logos</span> on ARM. This is why the following 
					benchmarks and tests will all target my Apple M1. 
					</p>

					<div class="blog-subtitle">Inlining Everything and Removing Branches</div>
					<p>
					Just out of curiosity: what happens when I place 
					<span class="ilc">#[inline(always)]</span> on every function?
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      240,993 ns/iter (+/- 3,382)
logos:     174,476 ns/iter (+/- 3,062)
```
</div>
					</div>

					<p>
					We seem to get a 30% speedup. I also noticed some API differences 
					between my lexer and logos: my lexer is by default "peekable", so every 
					invocation of <span class="ilc">next_token</span> contains a branch like this: 
					</p>

					<div class="blog-cg">
<div class="blog-code">
```rust
pub fn next_token(&mut self) -> Option<&Token<'a>> {
    if self.peeked.is_some() {
        self.current = self.peeked.clone();
        self.peeked = None;
        return self.current.as_ref();
    }
    // [...]
}
```
</div>
					</div>

					<p>
					Also, my token definition contains a slice:
					</p>

					<div class="blog-cg">
<div class="blog-code">
```rust
#[derive(Debug, Clone)]
pub struct Token<'a> {
    pub text: &'a [u8],
    pub kind: TokenKind,
}
```
</div>
					</div>

					<p>
					Having a fat pointer + enum, which is wrapped by an <span class="ilc">Option</span>
					is most definitely not memory-efficient.   After removing peeking logic, 
					the <span class="ilc">text</span> field, and switching from returning 
					refs to cloning, we're only 25% slower than logos:
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      220,825 ns/iter (+/- 947)
logos:     174,476 ns/iter (+/- 3,062)
```
</div>
					</div>

					<p>
					It's nothing to scoff at, but we probably need a more structured 
					approach to improving performance :) 
					</p>

					<div class="blog-subtitle">ASCII Dispatch</div>

					<p>
					We can also exploit the fact that <i>most of our source code is 
					actually ASCII</i>. Note that this is pretty common for source code 
					(except for <a href="https://en.wikipedia.org/wiki/APL_(programming_language)">APL</a>). 
					I ran a <a href="https://gist.github.com/dist1ll/4aeb98c045ab5c31edfc911a02f7a47d">test script</a>
					on a variety of codebases, and the results are pretty conclusive:
					</p>

					<p>
					<table>
						<thead>
							<tr>
								<th>Repo</th>
								<th>Lang</th>
								<th>LoC (incl. comments)</th>
								<th>ASCII%</th>
							</tr>
						</thead>
						<tbody>
							<tr><th>Linux</th><th>C</th><th>22 million</th><th>99.999%</th></tr>
							<tr><th>K8s</th><th>Go</th><th>5.2 million</th><th>99.991%</th></tr>
							<tr><th>Apache Spark</th><th>Scala</th><th>1.2 million</th><th>99.998%</th></tr>
							<tr><th>Hubris</th><th>Rust</th><th>114k</th><th>99.936%</th></tr>
						</tbody>
					</table>
					</p>

					<p>
					We can do an optimization which resembles what compilers do to vectorize 
					loops. In order to unroll a loop <span class="ilc">n</span> times, we 
					need to check if we can perform <span class="ilc">n</span> iterations 
					of work at the start of the loop. Depending on this check, we jump to 
					<i>either</i> the scalar <i>or</i> the vectorized implementation 
					(this is called dispatch). If we do this for ASCII, we get:
					</p>
					<div class="blog-cg">
<div class="blog-code">
```rust
if !c.is_ascii() { 
    return todo!("slow implementation"); 
}
/* rest of the code */
```
</div>
					</div>

					<p>
					After the if-statement, the compiler can safely assume that <span class="ilc">c</span> 
					is no larger than a single byte. This gives us pretty competitive results: 
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      206,928 ns/iter (+/- 2,724)
```
</div>
					</div>
					
					<p>
					However, <span class="ilc">todo!</span> terminates the program, which 
					may affect downstream optimizations. And indeed, if we replace the panic 
					with a proper function, we get a regression even with 
					<span class="ilc">#[inline(never)]</span>.
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:      243,292 ns/iter (+/- 7,678)
```
</div>
					</div>

					<p>
					So while it <i>can</i> bring benefits to dispatch on events with extreme
					probabilities, we have to do it in a way that plays nicely with automatic 
					vectorization; or we can avoid the dance with the compiler and do it ourselves.
					
					</p>

					<div class="blog-subtitle">Using SIMD Instructions</div>


					With ASCII dispatch in mind, we can begin to speed up the happy path. Assuming 
					all characters from now on are ASCII bytes, we can write a very simple 
					parallel lookup function. We do this by loading 16 consecutive bytes 
					into a 128-bit SIMD register, perform a table lookup, and store the results 
					into a temporary buffer that we'll use later. 
					<figure>
						<div class="blog-img blog-imgwide">
							<img alt="Rough sketch of our SIMD instructions" src="../static/fast-lexing/simd1.webp" style="max-height: 800px">
						</div>					
					</figure>

					<span class="sidenote">
					<b>Note</b>: All SIMD instructions here are ARMv8 NEON intrinsics, which are supported
					on all modern aarch64 systems. 
					You can find a breakdown of all instructions on 
					<a href="https://arm-software.github.io/acle/neon_intrinsics/advsimd.html">
					ARM's reference page</a>.
					</span>

					The rules for the lookup function <span class="ilc">f</span> are simple:

					<div class="blog-cg">
<div class="blog-code">
```ocaml
let f(x) = 
  | x is digit -> 0xfe
  | x is whitespace -> 0xfd
  | x is alphabetic -> 0xff
  | _ -> x
```
</div>
					</div>

					<p>
					Tokenizing single-symbol tokens (like <span class="ilc">+, -, *, /, :</span>) 
					is basically free, because we define the enum tag to be identical to its 
					corresponding ASCII code. 
					</p>

					<p>
					Alphanumeric and whitespace characters are the main problem, because their 
					associated tokens (identifiers, numbers and whitespace) can be arbitrarily long.
					</p>

					<p>
					Regarding the lookup: unfortunately, the instruction <span class="ilc">vqtbl4q_u8</span> 
					can only index into tables with a maximum size of 64 bytes. To translate 
					the entire ASCII space, we need to map the upper and lower half of the 
					ASCII space in two steps with two seperate tables, and then OR the results.
					</p>
					<figure>
						<div class="blog-img blog-imgwide">
							<img alt="Splitting lookup for dealing with tables larger than 64B" src="../static/fast-lexing/simd2.webp" style="max-height: 800px">
						</div>					
					</figure>

					<p>
					Once we have the fully mapped representation, we can execute what I call the 
					"skip loop". When we come across an alphabetic character (<span class="ilc">FF</span>),
					we know that this is the start of an identifier token. We must then 
					skip over all following alphanumeric characters until we hit the end 
					of the identifier. This boils down to a very simple and shallow state machine:
					</p>

					<div class="blog-cgw" style="flex-wrap: wrap-reverse;">
<div class="blog-code" style="margin-top: 25px; height: 100%; flex-grow: 1.5;">
```rust
let initial = buffer[simd_idx];
increment_simd_idx(1)?;

loop {
    let combined = (initial << 8) | buffer[simd_idx];
    let skip_cond = combined == 0xff_ff 
                 || combined == 0xff_fe 
                 || combined == 0xfe_fe 
                 || combined == 0xfd_fd;

    if !skip_cond {
        break;
    }
    increment_simd_idx(1)?;
}
```
</div>
					<figure style="min-width: 320px; width: 0; flex-grow: 1;">
						<div class="blog-img">
							<img alt="Skip loop represented as a small state machine" src="../static/fast-lexing/simd3.webp" style="max-height: 300px">
						</div>
						<div class="blog-caption">
							<b>Figure</b>: A visualization of our skip loop. Identifiers start 
							with an alphabetic character, and can continue with alphanumeric ones. 
							Whitespace and integer literals only accept 
							<span class="ilc">0xfd</span> and <span class="ilc">0xfe</span> respectively.   
						</div>
					</figure>
					</div>

					<p>
					As you can see, the transitions are a lot less complicated, because we 
					collapsed multiple range checks (e.g. "is x alphabetic?") into a single 
					branch (<span class="ilc">x == 0xff</span>). So while we're still performing 
					state machine looping over characters, we have 
					<b>effectively substituted the data dependency of the jump table 
					approach with a control dependency</b> (*).
					</p>
					
					<span class="sidenote">
						<style>@media (max-width: 940px) { .custom001 { float: right; }}</style>
						<style>@media (max-width: 500px) { .custom001 { float: none; }}</style>
						<div class="blog-img custom001" style="padding: 15px 0;">
							<img alt="Histogram showing the distribution of whitespace token lengths" src="../static/fast-lexing/distribution.webp" style="max-height: 220px;">
						</div>					
						<p>
						(*): That got me curious: how predictable are our multi-character tokens? 
						Above a histogram of whitespace tokens, categorized by their length. 
						(Whitespace is the most frequently occuring token)
						</p>
						<p>
						 It looks like the skip-loop branch is <b>very</b> predictable for 
						whitespace tokens. For identifiers, I'd need to lex some real-world code, 
						but identifiers generally have a much longer mean *and* tail. So while 
						the MIX of whitespace and identifiers in a character stream might be 
						less predictable, the <i>individual classes</i> most definitely are.
						</p>
					</span>

					<p>
					One potential problem with data 
					dependencies, and really with a lot of branchless algorithms, is that 
					they may reveal performance problems in the face of modern speculative 
					execution (e.g. due to not maxing out memory bandwidth or not benefiting 
					from branch prediction). We can run the benchmark again on our current
					implementation:
					</p>

					<div class="blog-cg">
<div class="blog-code">
```text
logos:     118,476 ns/iter (+/- 322)
```
</div>
					</div>
					<p>
					Our results are decent, getting a consistent speedup of 35% over logos 
					(for reproduction, see 
					<a href="https://github.com/dist1ll/lexer-exp/commit/94499e0df0f40bb492126c9f991dd8392f3f6fdd">4a9a1cd</a>). 
					</p>

					<div class="blog-subtitle">Lookup Table for Keyword Matching Mask</div>
					<p>
					Next, I revisited the keyword matching function looking for some low-hanging 
					fruits. I tried replacing my whacky multiply + add function with bit 
					extract instructions (in the spirit of <span class="ilc">pext</span>), 
					but that didn't seem to have any effect. 
					</p>

					<p>
					So I had a look at what we do <i>before</i> we push our bytes into the hash function. 
					To construct the 64-bit register that holds our data, we first load the input 
					characters into a register and then zero out the irrelevant bytes. For instance, the keyword 
					<span class="ilc">let</span> is three bytes long, so we zero out the other 
					five. This is done with a mask:
					</p>
					<div class="blog-cg">
<div class="blog-code">
```rust
let candidate = bytes & ((1 << len) - 1)
```
</div>
					</div>
					
					<p>
					This idiom is actually recognized by most optimizing compilers, and replaced 
					with the <span class="ilc">bzhi</span> instruction on x86 (assuming 
					support for BMI2) and <span class="ilc">lsl</span> + <span class="ilc">bic</span> 
					on ARM. But I was curious: what if we instead perform a lookup into a 
					table of precomputed masks?
					</p>
					<div class="blog-cg">
<div class="blog-code">
```rust
static bitmasks: [u64; 8] = [
    0x0000000000000000,
    0x00000000000000ff,
    0x000000000000ffff,
    0x0000000000ffffff,
    0x00000000ffffffff,
    0x000000ffffffffff,
    0x0000ffffffffffff,
    0x00ffffffffffffff,
];
// [...]
let candidate = candidate & bitmasks[len];
```
</div>
					</div>

					<p>
					To my surprise, the lookup table is actually faster! I didn't measure 
					this in detail, but it seemed to speed up keyword matching by 5-10%. 
					This turned out to not matter much, because keywords are not as frequent 
					in real-world data, but it's an interesting observation nevertheless. 
					</p>

					<div class="blog-subtitle">Getting More Realistic Data: Identifiers and Whitespaces</div>
					<p>
					With our implementation reaching an acceptable level of performance, we 
					can try to make our benchmarking data more realistic. To do that, I took 
					two snippets from Oxide's Hubris kernel and removed a few unsupported 
					characters. 
					The result: smaller number of multi-sequence tokens, but 
					identifiers and whitespaces are longer. On top of that, we now 
					allow underscores anywhere inside the identifier.
					All these things negatively affect our relative improvement over logos:
					</p>
					<div class="blog-cg">
<div class="blog-code">
```text
lnpl:       89,151 ns/iter (+/- 1,048)
logos:     108,257 ns/iter (+/- 1,898)
```
</div>
					</div>

					<p>
					I believe the main reason for this change is the fact that the 
					<a href="https://github.com/dist1ll/lexer-exp/blob/main/src/lexer/aarch64.rs#L188">
					branch that recognizes single-character tokens</a>
					has become less predictable. Another reason is the much smaller number 
					of keywords in our sample, decreasing the benefit of perfect hash 
					keyword matching. Finally, in order to support identifiers with 
					underscores, we needed to complicate our skip loop.
					</p>

					<p>
					Running benchmarks on file sizes of 5KB, 10KB, 50KB, 
					100KB and 1MB shows a speedup between 20%-30%. You can try out the
					<a href="https://github.com/dist1ll/lexer-exp">final version</a>
					for yourself (aarch64 only, run <span class="ilc">cargo bench</span>).
					</p>

					<span class="sidenote">
					<b>Credit</b>: Thanks to Mahmoud Mazouz for uncovering a performance
					regression while trying to reproduce the benchmark. The primary 
					culprit was the <span class="ilc">#[no_mangle]</span> attribute on the
					<span class="ilc">next_token</span> function,
					which I left in the code for debugging convenience.
					</span>
					
					<div class="blog-subtitle">Conclusion</div>
					This ends the first part into my experiments. We're able to beat logos 
					in pure throughput by at least 20%, presumably because our implementation 
					plays better with out-of-order and speculative execution when it's aggressively 
					inlined and unrolled. Still, it's unclear how we would perform when calls 
					to the lexer are interleaved in the parsing code. Even though we lose 
					loop unrolling, inlining our control-heavy function could allow individual 
					branches to become more predictable if the particular code path of the 
					parser path is statistically correlated to a predictable set of token patterns. 
					This idea reminded me of 
					<a href="http://www.cs.toronto.edu/~matz/dissertation/matzDissertation-latex2html/node6.html">threaded
					interpreters</a>, which duplicate dispatching code in
					the tail of every instruction handler, yielding a significant reduction
					in branch misses.

					<!-- +++++++++ -->
				</div>
			</div>
		</div>
	</body>
</html>
